%\subsection{Rate-Control Interface: \ksplit Design} 
\label{sec:design}
%\T{\ksplit.} 
We have developed a novel kernel module called \textit{\ksplit} %in Ubuntu 17.04, 
and have made it openly available on Github\footnote{URL and relevant citation hidden for double blind review}. \ksplit implements a kernel-based TCP split together with the four improvements of OCD \name over the OCD Baseline (\S\ref{sec:rate-control}). 
%In addition, \ksplit enables utilizing easily-deployable commodity VMs, as well as  standard programming APIs (POSIX/\sockets). %Specifically, it relies on 

%\T{Kernel mode.} We implemented \ksplit in kernel mode. We rely on procfs~\cite{proc} to control the behaviour of \ksplit. A virtual file system~\cite{virtfs} provides a simple interface and facilitates easy scripting; additionally, it allows to communicate in run time with \ksplit.

%The decision to use kernel mode is a significant one. While developing in user space would have provided an easier development environment, implementing \ksplit in the kernel allows us to (1) take advantage of resources only available in the kernel, such as \textit{Netfilter}~\cite{netfilter}, which is crucial to our needs; and (2) avoid the penalties that stem from numerous \textit{system calls}~\cite{Copy, FlexSC}. By working in the kernel, we eliminate the redundant transitions to and from user space and avoid gratuitous system calls. 


%The decision to implement the components of \ksplit in the kernel is further made easy by the fact that all socket APIs have kernel counterparts. One limitation of an in-kernel implementation is that epoll~\cite{epoll}, a scalable I/O event notification mechanism, has no kernel API. Instead, we use kernel\_threads to service our sockets. 
%We have measured the cost of context switching between kernel\_threads on our setup, assuming an n1-standard(GCP) VM with Intel Skylake Xeon CPU. We measured the time it takes two kernel\_threads to call schedule() 10 million times each. This experiment completes in under 3.2 seconds, resulting in 0.16 \usec per context switch on average; by comparison, an analogous experiment with two processes runs for 15.6 seconds and 12.9 seconds for two POSIX threads~\cite{pthreads}.
%Of course, these are not exactly comparable because the user-space experiments invoke system calls in order to switch context (sched\_yield()). In any case, since we want to avoid system calls and minimize context switches, implementing \ksplit in the kernel is the logical choice.

%\T{Basic implementation.} The basic implementation of \ksplit relies on three components. (1) We create a socket that listens for incoming connections. (2) Iptable~\cite{iptables} rules redirect specific TCP packets to our proxy socket. (3) A second TCP socket is used to connect to the destination and thus complete the second leg of the split connection. Once both connections are established, the bytes of a single stream are read from one socket, and then forwarded to its peer. This forwarding happens in both directions. When either connection is terminated via an error or FIN, the other connection is shut down as well. This means that the bytes in flight (\ie not yet acked) will reach their destination, but no new bytes can be sent.

%\T{Buffer size.} We found that the size of the buffer used to read and write the data is important. At first we used  a 4KB buffer, and experienced degraded performance. However, 16KB and 64KB maintain the same stable speed.

%\T{Implementing Early-SYN.} 
%As there is no standard API that enables the capture of the first SYN packet, we use Linux Netfilter~\cite{netfilter} hooks. We add a hook that captures TCP packets, and then parse the headers for the destination and the SYN flag. With this information \ksplit launches a new kernel\_thread\footnote{Creating a new thread while in the Netfilter callback is not allowed. We use our thread pool to lunch kernel\_threads from atomic contexts.} that initiates a connection to the intended destination. Capturing the SYN allows the \proxies to establish the two sides of a connection concurrently.

%\T{Implementing the thread pool.} 
%Each split connection is handled by two dedicated kernel\_threads~\cite{kthread}. Each thread receives from one socket and writes to its peer. One thread is responsible for one direction of the connection. We use blocking send/receive calls with our sockets allowing for a simple implementation; this also means that we need a kernel\_thread per active socket. Unfortunately, the creation of a new kernel\_thread is costly. On our setup, a kernel\_thread creation takes about 12\usec, on average.\footnote{By comparison a fork consumes more than 25\usec , while launching a POSIX pthread consumes around 13\usec.} But an outlier may consume several milliseconds, resulting in a jittery behaviour.

%To mitigate this problem and the problem of creating new kernel\_threads from atomic context, we create a pool of reusable threads. Each kernel\_thread in this pool is initially waiting in state TASK\_INTERRUPTIBLE (ready to execute). When the thread is allocated, two things happen: (1) a function to execute is set and (2)  the task is scheduled to run (TASK\_RUNNING). When the function is finished executing, the thread returns to state TASK\_INTERRUPTIBLE and back to the list of pending threads, awaiting to be allocated once more. A pool of pre-allocated kernel threads thus removes the overhead of new kernel\_thread creation. A new kernel\_thread from the waiting pool can start executing immediately and can be launched from any context. When the pool is exhausted, \ie all pool threads are running, a new thread will be allocated; thus for best performance the pool-size should be configured to cover the maximum possible number of concurrent connections. On a multi-core system, the heavy lifting of thread creation is offloaded to a dedicated core. This core executes a thread that allocates new kernel\_threads any time the pool size dips under some configurable value. On the other hand, when threads return to the pool, it is possible to conserve system resources by restricting the number of threads awaiting in the pool and freeing the execs threads.\footnote{Each kernel\_thread consumes 9KB of memory.}

%\T{Implementing \reconn.} 
%To implement \reconn, we have added a second server socket. Unlike the ``proxy'' socket, this socket listens for connections from other \proxies that are initiating new \reconn. In order to keep the connection from closing before it is allocated, the sockets are configured with KEEP\_ALIVE.

%When established, these connections await for the destination address to be sent from the initiating peer. The destination address is sent over the connection itself. This information is sent in the very first bytes, and all following bytes belong to the forwarded stream. 
%Once the destination address is received, a connection to the destination is established and the second leg of the split connection is established. The streams are forwarded between the sockets just like in the basic design.

%We found that Nagle's Algorithm~\cite{nagle} should be disabled on these sockets. In our experiments, we have seen that without disabling it, the time-to-first-byte is increased by some $200$ milliseconds.  

%\T{Proc.} The size of the thread-pool, the destination of a \reconn, and their number are controlled via the procfs~\cite{proc} interface.

%\T{Effort.} The total implementation of \ksplit is less than 2000 LOC (lines of code). The basic implementation is about 500 LOC, thread pool and early syn add 300 LOC each, and \reconn add 500 LOC. The code for the proc interface and common utilities consists of about 300 LOC.

%\T{Implementation at scale.} We now want to briefly discuss how the existing implementation may be scaled in the future. With 10K split connections, the memory footprint of socket buffers alone; far exceeds the size of the shared L3 cache of most modern servers\footnote{On GCP, it is an impressive 56MB.}. It may be prudent to expand the epoll API to the kernel and thus save the 18KB of memory per split connection. Extending epoll will not be enough, other avenues should be considered as well.
%One such idea is the socket API; socket API in the kernel is not zero copy. The needless copy can become detrimental ~\cite{Copy} due to an increase in costly memory traffic. Another point to consider is that, network I/O is serviced by interrupts. For a virtual machine, this means expensive VM exits~\cite{Eli, Elvis}. It is well documented that para-virtual devices like~\cite{virtio,vmxnet3} have sub-optimal performance~\cite{Eli, Elvis}. An SRIOV device and a machine with a CPU that supports Intel's vt-d posted interrupts~\cite{posted} may be needed to achieve near bare-metal performance.